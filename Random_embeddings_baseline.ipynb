{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYuB7tOL/voaimoVk/pDJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukasStankevicius/Random-embeddings-baseline-for-text-level-NLP-tasks/blob/main/Random_embeddings_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AeAPlfO-b68O"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "class RandomEmbeddingsBaseline:\n",
        "    def __init__(self, model_name, vector_size, mu=0, sigma=0.1, random_state=0):\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "        self.tested_models = ['bert-base-uncased', \"t5-small\", \"google/mt5-small\", \"google/byt5-small\", \"meta-llama/Llama-2-7b-hf\"]\n",
        "\n",
        "        if model_name == \"lowersplit\":\n",
        "            self.word2id = {}\n",
        "            self.word2id = defaultdict(lambda: len(self.word2id))\n",
        "            self.tokenizer = lambda text: {'input_ids': [self.word2id[i] for i in text.lower().split(' ')]}\n",
        "            self.tokenizer_kwargs = {}\n",
        "            n_tokens = 750_000\n",
        "        elif model_name in self.tested_models:\n",
        "            use_fast = model_name in [\"google/mt5-small\"]\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n",
        "            self.tokenizer_kwargs = dict(add_special_tokens=False)\n",
        "            n_tokens = self.tokenizer.vocab_size\n",
        "        else:\n",
        "          raise ValueError('Untested model')\n",
        "\n",
        "        self.vectors = self.rng.normal(mu, sigma, (n_tokens, vector_size))\n",
        "        self.vectors = self.vectors.astype('float32')\n",
        "\n",
        "    def text_to_vector(self, text):\n",
        "      token_indexes = self.tokenizer(text, **self.tokenizer_kwargs)['input_ids']\n",
        "      sentence_vector = self.vectors[token_indexes].mean(axis=0) if token_indexes else np.zeros(shape=(self.vectors.shape[1]), dtype='float32')\n",
        "      return sentence_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = RandomEmbeddingsBaseline(model_name='bert-base-uncased', vector_size=48)\n",
        "text = \"This is the test sentence.\"\n",
        "print(m.text_to_vector(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQRE9oaDtV8K",
        "outputId": "7f26b9bd-e78c-4eb4-ae58-e5a224ba7513"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01570123  0.05268576 -0.01440263  0.01644494 -0.02822436  0.00737305\n",
            "  0.00538737 -0.00791293  0.00618587 -0.05503013 -0.01529175 -0.12204976\n",
            "  0.01475287 -0.02839016 -0.05140474 -0.09164638  0.00553092  0.09184373\n",
            " -0.03690748  0.02093396  0.00842    -0.04734223 -0.02477467  0.01362352\n",
            "  0.0004928  -0.03422094  0.06709204 -0.02618556 -0.00178645 -0.08016231\n",
            " -0.07860541  0.01794009  0.03173523 -0.02214176 -0.01232245  0.00542088\n",
            " -0.00080929  0.05717109  0.01065253  0.02649156  0.05100659  0.08113872\n",
            " -0.01249488  0.02838389 -0.00670874  0.00496173  0.02301297 -0.03081081]\n"
          ]
        }
      ]
    }
  ]
}